name: run crawler_1

on:
  # 주기적 실행 매일 UTC 00:00 (한국 09:00)
  schedule:
    - cron: '00 23 * * *'
    - cron: '10 23 * * *'
    - cron: '20 23 * * *'
    - cron: '30 23 * * *'
    - cron: '40 23 * * *'
  # 수동 실행
  workflow_dispatch:

jobs:
  crawler_1:
    name: build-push-run-upload
    runs-on: ubuntu-latest
    permissions:
      contents: read #repo 코드 접근
      packages: write #GHCR push 권한
    env:
      LOG_LEVEL: INFO

    steps:
      # UTC 시간과 runner의 트리거 시간 확인
      - name: Check runner time
        run: |
          echo "UTC: $(date -u)"
          echo "KST: $(TZ=Asia/Seoul date)"
          echo "runner: $(date)"

      # GitHub repo의 코드를 runner VM으로 복사
      - name: Checkout repository
        uses: actions/checkout@v4

      # Docker Buildx 세팅
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # GHCR 로그인
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Docker 이미지 빌드 + GHCR push
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: . # 빌드 context / 현재 경로를 기준으로 파일 참조
          file: ./Dockerfile # Dockerfile 경로
          push: true # 빌드 후 GHCR로 푸시
          tags: |
            ghcr.io/trillion-impulse/project_python-web-crawler-1/crawler_1:latest
            ghcr.io/trillion-impulse/project_python-web-crawler-1/crawler_1:${{ github.sha }}
          build-args: |
            LOG_LEVEL=${{ env.LOG_LEVEL }}
      
      # output 전용 디렉토리 생성 (Dockerfile의 WORKDIR에 의존하지 않도록)
      - name: output directory
        run: mkdir -p output

      # Docker 컨테이너 실행 (Secret 환경변수는 여기서 주입)
      - name: Run crawler inside Docker
        run: |
          docker run --rm \
            -e CRAWLER_1_URL="${{ secrets.CRAWLER_1_URL }}" \
            -e OUTPUT_DIR="/output" \
            -v ${{ github.workspace }}/output:/output \
            ghcr.io/trillion-impulse/project_python-web-crawler-1/crawler_1:latest

      # Artifact 업로드
      - name: Upload CSV to Artifact
        uses: actions/upload-artifact@v4
        with:
          name: crawler_1-output
          path: output/*.csv